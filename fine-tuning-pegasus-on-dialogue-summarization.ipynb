{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9119752,"sourceType":"datasetVersion","datasetId":5505144}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1- Importing Libs.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate rouge-score\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, pipeline\nimport torch\nfrom tqdm import tqdm\nimport evaluate\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:31:29.845508Z","iopub.execute_input":"2024-12-12T10:31:29.845793Z","iopub.status.idle":"2024-12-12T10:32:11.683175Z","shell.execute_reply.started":"2024-12-12T10:31:29.845766Z","shell.execute_reply":"2024-12-12T10:32:11.682413Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=08372434f77f432ed26b5126996eb4e8b92a4b533d335c2e939a48e002d0444a\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 2. Preparing environment","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:33:36.306757Z","iopub.execute_input":"2024-12-12T12:33:36.307383Z","iopub.status.idle":"2024-12-12T12:33:36.426123Z","shell.execute_reply.started":"2024-12-12T12:33:36.307348Z","shell.execute_reply":"2024-12-12T12:33:36.425223Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"\nos.environ['HF_TOKEN']=secret_value_0\n\nlogin(token=os.getenv('HF_TOKEN'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:33:37.173238Z","iopub.execute_input":"2024-12-12T12:33:37.173567Z","iopub.status.idle":"2024-12-12T12:33:37.223786Z","shell.execute_reply.started":"2024-12-12T12:33:37.173536Z","shell.execute_reply":"2024-12-12T12:33:37.223071Z"}},"outputs":[{"name":"stderr","text":"Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"model_ckpt = \"google/pegasus-cnn_dailymail\"\ndevice = 'cuda' if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:32:17.364481Z","iopub.execute_input":"2024-12-12T10:32:17.365061Z","iopub.status.idle":"2024-12-12T10:32:17.462518Z","shell.execute_reply.started":"2024-12-12T10:32:17.365028Z","shell.execute_reply":"2024-12-12T10:32:17.461502Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 3. loading and taking a look at the dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"knkarthick/dialogsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:32:19.997264Z","iopub.execute_input":"2024-12-12T10:32:19.997832Z","iopub.status.idle":"2024-12-12T10:32:22.277981Z","shell.execute_reply.started":"2024-12-12T10:32:19.997799Z","shell.execute_reply":"2024-12-12T10:32:22.277023Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1595a9f5f5e34e29aadf91e167ed3f83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"318db25e44ca4064b250b6750f395ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea8178150bd43e18059d4c2a994b39e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43dfc06e3a4c42a9adfa82153bc75e22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6160802da54bf48faa1efd83016421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167543b5c1bb4f4591516fd793ea3493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab3ff065d9e6490ab1197b0d24cd9fbb"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:32:26.552216Z","iopub.execute_input":"2024-12-12T10:32:26.552572Z","iopub.status.idle":"2024-12-12T10:32:26.558344Z","shell.execute_reply.started":"2024-12-12T10:32:26.552533Z","shell.execute_reply":"2024-12-12T10:32:26.557546Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print(dataset['train']['dialogue'][0], dataset['train']['summary'][0], sep='\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:34:29.772322Z","iopub.execute_input":"2024-12-12T10:34:29.773142Z","iopub.status.idle":"2024-12-12T10:34:29.805606Z","shell.execute_reply.started":"2024-12-12T10:34:29.773107Z","shell.execute_reply":"2024-12-12T10:34:29.804753Z"}},"outputs":[{"name":"stdout","text":"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n#Person2#: I found it would be a good idea to get a check-up.\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n#Person2#: Ok.\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n#Person2#: Yes.\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n#Person2#: Ok, thanks doctor.\n\nMr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 4. Evaluating the model before fine tuning","metadata":{}},{"cell_type":"code","source":"metric = evaluate.load(\"rouge\")\n\ndef chunks(list_of_elements, batch_size):\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\ndef evaluate_summaries(model, \n                       tokenizer, \n                       dataset, \n                       batch_size, \n                       metric, \n                       col_name='dialogue', \n                       col_summary='summary'):\n    article_batches = list(chunks(dataset[col_name], batch_size))\n    summary_batches = list(chunks(dataset[col_summary], batch_size))\n\n    for article_batch, summary_batch in tqdm(zip(article_batches, summary_batches), total=len(article_batches)):\n        inputs = tokenizer(article_batch, max_length=1024, \n                           truncation=True, return_tensors='pt', padding='max_length')\n        summaries = model.generate(input_ids=inputs['input_ids'].to(device), \n                                   attention_mask=inputs['attention_mask'].to(device), \n                                   max_length=128, length_penalty=0.8, num_beams=5)\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                              clean_up_tokenization_spaces=True) \n                             for s in summaries]\n        decoded_summaries = [d.replace('<n>', \" \") for d in decoded_summaries]\n        metric.add_batch(predictions=decoded_summaries, references=summary_batch)\n\n    score = metric.compute()\n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:35:36.274412Z","iopub.execute_input":"2024-12-12T10:35:36.275328Z","iopub.status.idle":"2024-12-12T10:35:37.548474Z","shell.execute_reply.started":"2024-12-12T10:35:36.275282Z","shell.execute_reply":"2024-12-12T10:35:37.547724Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbbf6d79cd3744a993045a49e806dbf6"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\nscore = evaluate_summaries(model, tokenizer, dataset['validation'][:100], 4, metric)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:38:19.311208Z","iopub.execute_input":"2024-12-12T10:38:19.311535Z","iopub.status.idle":"2024-12-12T10:40:00.569917Z","shell.execute_reply.started":"2024-12-12T10:38:19.311507Z","shell.execute_reply":"2024-12-12T10:40:00.568797Z"}},"outputs":[{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 25/25 [01:34<00:00,  3.76s/it]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\npd.DataFrame(rouge_dict, index=[\"PEGASUS\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:40:00.571379Z","iopub.execute_input":"2024-12-12T10:40:00.572365Z","iopub.status.idle":"2024-12-12T10:40:00.590005Z","shell.execute_reply.started":"2024-12-12T10:40:00.572321Z","shell.execute_reply":"2024-12-12T10:40:00.589029Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"           rouge1    rouge2    rougeL  rougeLsum\nPEGASUS  0.261658  0.067741  0.201517    0.20154","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PEGASUS</th>\n      <td>0.261658</td>\n      <td>0.067741</td>\n      <td>0.201517</td>\n      <td>0.20154</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# 5. Fine Tuning PEGASUS","metadata":{}},{"cell_type":"code","source":"def tokenize(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], truncation=True, max_length=128)\n\n    return {\n        \"input_ids\": input_encodings['input_ids'], \n        \"attention_mask\": input_encodings['attention_mask'], \n        \"labels\": target_encodings['input_ids']\n    }\n\n\ntrain_dataset = dataset['train'].map(tokenize, batched=True) \nval_dataset = dataset['validation'].map(tokenize, batched=True) \ntest_dataset = dataset['test'].map(tokenize, batched=True) \n\ncolumns = ['input_ids', 'labels', 'attention_mask']\ntrain_dataset.set_format(type='torch', columns=columns)\nval_dataset.set_format(type=\"torch\", columns=columns)\ntest_dataset.set_format(type=\"torch\", columns=columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:13:18.167735Z","iopub.execute_input":"2024-12-12T11:13:18.168396Z","iopub.status.idle":"2024-12-12T11:13:23.526256Z","shell.execute_reply.started":"2024-12-12T11:13:18.168361Z","shell.execute_reply":"2024-12-12T11:13:23.525530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bbdff62afe9498d8400b0a99eb14c66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773dd9e92cff4189855eaa1abdaaa17e"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:43:30.501812Z","iopub.execute_input":"2024-12-12T11:43:30.502726Z","iopub.status.idle":"2024-12-12T11:43:30.507208Z","shell.execute_reply.started":"2024-12-12T11:43:30.502665Z","shell.execute_reply":"2024-12-12T11:43:30.506354Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"training_arguments = Seq2SeqTrainingArguments(\n    output_dir='pegasus-dialogue',  \n    num_train_epochs=1,\n    warmup_steps=500,\n    per_device_train_batch_size=1, \n    per_device_eval_batch_size=1,\n    weight_decay=0.01, \n    logging_steps=50, \n    push_to_hub=True, \n    eval_strategy='steps', \n    eval_steps=1000, \n    save_steps=1e6, \n    gradient_accumulation_steps=16,\n    predict_with_generate=True,\n    report_to=[]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:33:34.202402Z","iopub.execute_input":"2024-12-12T12:33:34.202765Z","iopub.status.idle":"2024-12-12T12:33:34.238501Z","shell.execute_reply.started":"2024-12-12T12:33:34.202734Z","shell.execute_reply":"2024-12-12T12:33:34.237698Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(model=model, \n                         tokenizer=tokenizer, \n                         args=training_arguemnts, \n                         data_collator=seq2seq_data_collator, \n                         train_dataset=train_dataset.select(range(6000)), \n                         eval_dataset=val_dataset\n                        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:33:37.800433Z","iopub.execute_input":"2024-12-12T12:33:37.801236Z","iopub.status.idle":"2024-12-12T12:33:38.016284Z","shell.execute_reply.started":"2024-12-12T12:33:37.801203Z","shell.execute_reply":"2024-12-12T12:33:38.015605Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/963016224.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(model=model,\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:33:38.067185Z","iopub.execute_input":"2024-12-12T12:33:38.067965Z","iopub.status.idle":"2024-12-12T13:41:38.071088Z","shell.execute_reply.started":"2024-12-12T12:33:38.067919Z","shell.execute_reply":"2024-12-12T13:41:38.070171Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='374' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [374/374 1:06:52, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=374, training_loss=1.6713089904683158, metrics={'train_runtime': 4022.8018, 'train_samples_per_second': 2.983, 'train_steps_per_second': 0.093, 'total_flos': 8679313386455040.0, 'train_loss': 1.6713089904683158, 'epoch': 1.9946666666666668})"},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"# 6. Evaluating on the validation set","metadata":{}},{"cell_type":"code","source":"score = evaluate_summaries(trainer.model, tokenizer, dataset['test'][:100], 4, metric)\nrouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\npd.DataFrame(rouge_dict, index=[\"fine-tuned-PEGASUS\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:44:21.547904Z","iopub.execute_input":"2024-12-12T13:44:21.548592Z","iopub.status.idle":"2024-12-12T13:45:32.445095Z","shell.execute_reply.started":"2024-12-12T13:44:21.548559Z","shell.execute_reply":"2024-12-12T13:45:32.444227Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 25/25 [01:10<00:00,  2.83s/it]\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"                      rouge1    rouge2    rougeL  rougeLsum\nfine-tuned-PEGASUS  0.366658  0.119804  0.289407   0.290264","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fine-tuned-PEGASUS</th>\n      <td>0.366658</td>\n      <td>0.119804</td>\n      <td>0.289407</td>\n      <td>0.290264</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"trainer.push_to_hub('pegasus-dialogsum')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:52:27.872656Z","iopub.execute_input":"2024-12-12T13:52:27.873070Z","iopub.status.idle":"2024-12-12T13:52:42.248687Z","shell.execute_reply.started":"2024-12-12T13:52:27.873039Z","shell.execute_reply":"2024-12-12T13:52:42.247819Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Ahmed167/pegasus-dialogue/commit/755c1e623fb3a405b38b5daf9d3d826e7322e250', commit_message='pegasus-dialogsum', commit_description='', oid='755c1e623fb3a405b38b5daf9d3d826e7322e250', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Ahmed167/pegasus-dialogue', endpoint='https://huggingface.co', repo_type='model', repo_id='Ahmed167/pegasus-dialogue'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"## A sample of the test set","metadata":{}},{"cell_type":"code","source":"pipe = pipeline('summarization', model=\"Ahmed167/pegasus-dialogue\")\nsample = train_dataset.select(range(1))\ndialogue = sample['dialogue']\nreference = sample['summary']\n\nprint(f\"Dialogue:\\n{dialogue[0]}\")\nprint(\"*\"*20)\nprint(f\"model summary:\\n{pipe(dialogue, length_penalty=0.8, num_beams=8, max_length=128)[0]['summary_text']}\")\nprint(\"*\"*20)\nprint(f\"reference:\\n{reference[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:05:51.524492Z","iopub.execute_input":"2024-12-12T14:05:51.525297Z","iopub.status.idle":"2024-12-12T14:06:06.104260Z","shell.execute_reply.started":"2024-12-12T14:05:51.525263Z","shell.execute_reply":"2024-12-12T14:06:06.103358Z"}},"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\n#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n#Person2#: I found it would be a good idea to get a check-up.\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n#Person2#: Ok.\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n#Person2#: Yes.\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n#Person2#: Ok, thanks doctor.\n********************\nmodel summary:\nMr. Smith visits Doctor Hawkins to get a check-up. Doctor Hawkins tells Mr. Smith smoking is the leading cause of lung cancer and heart disease.\n********************\nreference:\nMr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n","output_type":"stream"}],"execution_count":62}]}